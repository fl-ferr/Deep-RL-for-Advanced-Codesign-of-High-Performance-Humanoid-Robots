\chapter{Proximal Policy Optimization}

\begin{algorithm}[H]
    \caption{Clipped Proximal Policy Optimization}
    \label{alg:ppo}
    \begin{algorithmic}[1]
    \REQUIRE Initial policy parameters $\theta _0$, initial value
    \FOR{$k = 0,1,2, \dots$}
    \STATE{Collect set of trajectories $\mathcal{D} _k = \tau _i$ by running policy $\pi _k = \pi(\theta _k)$in the environment}
    \STATE{Compute rewards-to-go $\hat{R} _t$}
    \STATE{Compute advantage estimates $\hat{A} _t$
(using any method of advantage estimation) based on the current value function $V _{\phi _k}$}
    \STATE{Update policy by maximizing the PPO-Clip objective:}
    $$
\theta _{k + 1} = \arg\max _{\theta} = \frac{1}{|\mathcal{D} _k|T} \sum _{r \in \mathcal{D} _k} \sum _{t = 0} ^{T} \min \left( \frac{\pi _{\theta} (a _t | s _t)}{\pi _{\theta_k} (a _t | s _t)} A ^{\pi _{\theta_k}} (s _t, a _t), g(\varepsilon, A ^{\pi _{\theta_k}}(s _t, a _t)) \right)
    $$    
    typically via Stochastic gradient ascent with Adam. Where:
    $$
\hat{g} = \hat{\mathbb{E}} _t \left[\nabla _{\theta}\log\pi _{\theta}(a _t | s _t) \hat{A} _t\right]
$$
    \STATE{Fit value function by regression on mean-squared error}
    $$
    \phi _{k + 1} = \arg\min _{\phi} = \frac{1}{|\mathcal{D} _k|T} \sum _{r \in \mathcal{D} _k} \sum _{t = 0} ^{T} \left(V _{\phi}(s _t) - \hat{R} _t \right)^2
$$
typically via some gradient descent algorithm.
    \ENDFOR 
    \end{algorithmic}
\end{algorithm} 

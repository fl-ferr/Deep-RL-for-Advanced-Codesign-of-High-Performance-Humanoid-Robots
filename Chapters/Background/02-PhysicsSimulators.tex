\chapter{Hardware Accelerated Physics Simulation Frameworks}
\label{chp:back_PhysicsSimulators}

Training a physical \ac{RL} agent often involves prohibitive costs and potential safety issues, that is why simulators play a major role in most training scenarios. As a matter of fact, the trial-and-error process needed for the agent to gain experience regarding the world may involve damaging complex and expensive hardware, limiting the possibility for the agent to explore and learn more efficiently. With the use of scalable physics simulators, there is the possibility of creating highly complex, customized environments to reproduce multiple scenarios, allowing also to have a testing environment that is as close as possible to the real world. Nevertheless, with the increase of the environment and agent complexity,

At time, the most common physic simulation framework used for this purpose are PyBullet \citep{coumans_pybullet_2016}, MuJoCo \citep{todorov_mujoco_2012},

The most common way to speed up computations is to use hardware accelerators, in fact, the natural efficiency of \ac{GPU}s in solving parallel calculations can be exploited to cut down simulation times \citep{liang_gpu-accelerated_2018}.

\section{Nvidia ISAAC Gym}

In recent times, the use of \textit{Advesarial Motion Priors} \citep{peng_amp_2021} for reinforcement learning, which will be further discussed in \cref{chp:back_RLGA} has brought to the development of a new framework for physics simulation, called \textsc{ISAAC Gym} \citep{makoviychuk_isaac_2021} which allows to simulate complex environments basing on the \textsc{ISAAC SIM} \citep{zhou_towards_2023}, a cyber-physical simulator that exploits NVIDIA PhysX and Omniverse, which offer a high-performance, cross-platform, real-time physics engine that allows to simulate rigid bodies in reduced coordinate articulations, soft bodies, fluids, cloth and particles. Isaac Gym then leverages the power of \textit{RL Games} \citep{rl-games2021} to completely work with \textit{PyTorch} \citep{paszke_pytorch_2019} tensors on \ac{GPU}s to accelerate the simulation process, allowing to train complex environments with multiple agents and objects.

The backend implementation of the simulator is written in \cpp, while offering a frontend interface in Python, which allows to easily build complex environments and agents, as well as to visualize the simulation process. Nevertheless, being the framework completely closed-source, it is not possible to extend the functionalities of the simulator, which is limited to the ones provided by the original developers.

\section{JAX: High-Performance Array Computing}

When it comes to high-demanding computations, the use of \ac{GPU}s is often the preferred choice, yet that requires in most of the cases to write low-level code in \ac{CUDA}, \cpp or other low-level languages. This is not always the most immediate choice for rapid prototyping as it requires a lot of time to be spent on implementation, debugging, and final code polishing. Moreover, the use of low-level languages often leads to less readable code, which is not always the best choice when it comes to sharing the code with other researchers or developers. Amongst the variety of framework that offers a high-level interface to make it easier for the user to write code that can be run on \ac{GPU}s, \jax \citep{bradbury_jax_2018,47008} is rapidly becoming one of the most popular choices. It exploits the power of \ac{XLA} \citep{50530}, which is a domain-specific, just-in-time, graph-based compiler for linear algebra that leverages efficient kernel fusion and lazy tensor materialization, firstly developed for TensorFlow \citep{tensorflow2015-whitepaper} and then extended to PyTorch and \jax itself.
Computations on \ac{CPU} also benefit from the use of \ac{JIT} compilation, which allows to compile the code at runtime and fused operations \citep{wang_kernel_2010,snider_operator_2023}, which allows to combine multiple algebraic operations into a single \textit{Fused Multiply-Add} operation(\ac{FMA}), hence reducing the overhead of the compilation process and avoiding intermediate results by rounding the result of the multiplication to the nearest representable number in the given precision.
Moreover, \jax supports back and forward \textit{Automatic Differentiation} (\ac{AD}), which is a key feature for the implementation of deep learning algorithms, physical system modeling, and optimization.

\section{JAXSim}

\jaxsim \citep{ferigo_jaxsim_2022} as it is \ac{JIT} and leverages \jax
It provides an end-to-end GPU-accelerated simulation that allows strong parallelization on multiple environments, whose only limit is the number of \ac{CUDA} cores present in the GPU unit.

\chapter{Conclusions and Future Work}
\label{chp:contrib_Conclusions}

The process of implementing a codesign loop has required several intermediate steps. First, the review and adaptation of an already established recursive algorithm for the forward dynamics computation, which allowed to study the effect of the motor dynamics in robotics simulations, then the application of deep reinforcement learning to two main problems, a simple balancing task with a cartpole system, which required the development of an interface between a hardware accelerated simulator like \jaxsim and a reinforcement learning baseline implementation, and a more complex task of locomotion with a humanoid robot, which will represent the major challenge for the future development of the framework. Finally, the implementation of an evolutionary algorithm capable of exploiting synthetic data generated in the reinforcement learning framework in order to optimize the motor parameters of a robot, which allowed to study the effects of the motor design on the robot's performance.

The modified \ac{ABA} algorithm allows for a more realistic forward dynamics computation when there is the need to take into account the motor dynamics while keeping the same computational complexity $\mathcal{O}^n$, making it suitable for real-time applications. However, some hypotheses are still made in the derivation of the algorithm. In particular, future versions of the algorithm might take into account the effect of a non-negligible transmission inertia and the effect of Coulomb friction, which is not considered in the current implementation. Furthermore, eliminating the assumption of equal motion subspace for the motor and the link would allow for the computation of more complex kinematic chains involving, for example, internal kinematic loops or additional degrees of freedom.

The outcomes derived from the reinforcement learning framework demonstrate the efficacy of the suggested approach in solving the tasks considered in this work. However, future versions of \jaxsim might include a more effective and accurate contact model, perhaps exploiting the differentiability of \jax, which has not been extensively studied yet. Moreover, the addition of a more accurate contact model and the development of an integrated visualizer would ease its use as a reinforcement learning playground in combination with the emerging differentiable physics engines.
The framework of \jax could be also exploited to have an open-source version of Adversarial Motion Prior, which exploits the differentiability to easily adapt learned policies to new robots. This would allow for a more effective transfer of learned experience between different robots, which is a crucial aspect in the development of robotic systems.

Finally, the results obtained with the robot codesign process show that the proposed approach is effective in finding a motor combination that is able to maximize the reward. As a matter of fact, we can conclude that small motors were preferable for the chosen task. Hence, the stabilization effects coming from the higher inertia and viscous friction for the prismatic joint might potentially be beneficial in order to maximize the reward during the training. However, the results obtained with the genetic algorithm are limited to a discrete motor parameter set. In the future, supposing to have an unlimited choice for motor design, the codesign loop might take into account continuous search spaces for the motor parameters, which would allow for more in-depth optimization. Moreover, the codesign loop might be extended not only to include the design of the robot morphology parameters, such as the link lengths and density, enabling the algorithm to find the optimal robot design for a given task also from a geometrical point of view, but also to exploit multiple objectives in the optimization process in order to find a trade-off between different aspects of the robot design, such as the energy consumption and the robustness of the robot to external perturbations via the use of a multi-objective genetic algorithm like \ac{NSGA}-III.
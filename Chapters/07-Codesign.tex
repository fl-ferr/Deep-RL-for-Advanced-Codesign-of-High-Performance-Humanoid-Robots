\chapter{Humanoid Robots Codesign}
\label{chp:07-Codesign}



\section{Reinforcement Learning Problem Definition}

\lipsum[1]

\begin{table}[]
    \centering
    \begin{tabular}{l c}
        \toprule
        Space & Projection Space \\
        \midrule
        Action Space $\mathcal{A}$ & $\mathbb{R} ^{23}$\\
        Observation Space $\mathcal{S}$ & $\mathbb{R} ^{80}$ \\
        Reward Space $\mathcal{R}$ & $[0,1]$ \\
        \bottomrule
    \end{tabular}
    \caption{RL Framework Space Definition}
    \label{tab:rlspacedef}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{c}{\textbf{Training Parameters}} \\
Time Horizon & $2048$ \\
Minibatch size & $4096$ \\
Num Minibatch & $32$ \\
Number of Epochs & $10$ \\
\midrule
\multicolumn{2}{c}{\textbf{PPO Algorithm Parameters}} \\
Clipping & $0.3$ \\
KL Target & Not Implemented \\
KL Initialization & Not Implemented \\
Discount Factor $\gamma$ & $0.99$ \\
GAE Parameter $\lambda$ & $0.95$ \\
\midrule
\multicolumn{2}{c}{\textbf{Coefficients}} \\
Value Function Coefficient & $1.0$ \\
Entropy Coefficient $\beta$ & $0.01$ \\
\midrule
\multicolumn{2}{c}{\textbf{Optimization Parameters}} \\
Learning Rate & $0.0003$ \\
Maximum Gradient Norm & $0.5$ \\
\bottomrule
\end{tabular}
\caption{PPO Hyperparameters}
\end{table}


\section{Evolutionary Algorithm Problem Definition}


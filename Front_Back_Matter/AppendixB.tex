\section{RL walking task with JAXsim}
\label{sec:rlwalkingtaskwithjaxsim}

The action space is defined as:

\begin{equation}
    \mathcal{A} = \left\{ a \in \mathbb{R} ^{23} \mid -1 \leq a_i \leq 1 \right\}
\end{equation}

where $a_i$ is the $i$-th joint desired position, which is then converted to a torque via a PD controller and then clipped to the maximum torque of the joint, while the observation space is defined as:

\begin{equation}
    \mathcal{S} = \left\{ s \in \mathbb{R} ^{80} \mid -\infty \leq s_i \leq \infty \right\}
\end{equation}

and it is composed of the elements listed in \cref{tab:walkingobs_jaxsim}.

\begin{table}
    \centering
    \label{tab:walkingobs_jaxsim}
    \begin{tabular}{l c}
        \toprule
        \textsc{Observation} & \textsc{Dimension} \\
        \midrule
        Base Position        & $\mathbb{R} ^{3}$  \\
        Joints Positions     & $\mathbb{R} ^{23}$ \\
        Joints Velocities    & $\mathbb{R} ^{23}$ \\
        Actuation Torques    & $\mathbb{R} ^{23}$ \\
        Contact Points       & $\mathbb{R} ^{1}$  \\
        Target Position      & $\mathbb{R} ^{3}$  \\
        \bottomrule
    \end{tabular}
    \caption{Humanoid Walking Task -- Observation Space}
\end{table}

Rewards, actions, and observations are then normalized to the range $[0,1]$ to make the learning process more stable and to avoid the need to tune the reward function for different robots. The \cref{tab:walkingtaskspacedef} summarizes the definition of the spaces used in the \ac{RL} framework.

\begin{table}
    \centering
    \begin{tabular}{l c}
        \toprule
        \textsc{Space}                  & \textsc{Projection Space} \\
        \midrule
        Action Space $\mathcal{A}$      & $\mathbb{R} ^{23}$        \\
        Observation Space $\mathcal{S}$ & $\mathbb{R} ^{80}$        \\
        Reward Space $\mathcal{R}$      & $[0,1]$                   \\
        \bottomrule
    \end{tabular}
    \caption{Humanoid Walking Task -- Space Definition}
    \label{tab:walkingtaskspacedef_jaxsim}
\end{table}


Hyperparameter tuning is a crucial part of the \ac{RL} framework, and it is often the most time-consuming part of the process. In this work, the hyperparameters were tuned using \texttt{optuna} \citep{akiba_optuna_2019}, a hyperparameter optimization framework, which uses \textit{Tree-structured Parzen Estimator} (\ac{TPE}) sampler with a median pruner, that models the objective function and suggests the next set of hyperparameters to evaluate. The hyperparameters tuned resulted in the values reported in \cref{tab:ppohyperparameters_jaxsim}.


\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textsc{Parameter}          & \textsc{Value}          \\
        \midrule
        \multicolumn{2}{c}{\textbf{Training Parameters}}      \\
        Time Horizon                & $2048$                  \\
        Minibatch size              & $4096$                  \\
        Num Minibatch               & $32$                    \\
        Number of Epochs            & $10$                    \\
        \midrule
        \multicolumn{2}{c}{\textbf{PPO Algorithm Parameters}} \\
        Clipping                    & $0.3$                   \\
        KL Target                   & Not Implemented         \\
        KL Initialization           & Not Implemented         \\
        Discount Factor $\gamma$    & $0.99$                  \\
        GAE Parameter $\lambda$     & $0.95$                  \\
        \midrule
        \multicolumn{2}{c}{\textbf{Coefficients}}             \\
        Value Function Coefficient  & $1.0$                   \\
        Entropy Coefficient $\beta$ & $0.01$                  \\
        \midrule
        \multicolumn{2}{c}{\textbf{Optimization Parameters}}  \\
        Learning Rate               & $0.0003$                \\
        Maximum Gradient Norm       & $0.5$                   \\
        \bottomrule
    \end{tabular}
    \caption{Proximal Policy Optimization Hyperparameters}
    \label{tab:ppohyperparameters_jaxsim}
\end{table}

Given the high complexity of the problem, the network architecture composed of two separate networks, one for the policy and one for the value function, has been enlarged in order to increase the capacity of the network to learn the task and to avoid the need of a large number of training iterations.

The policy network is composed of two hidden layers of size $64$ and $64$ respectively, with a \ac{ReLU} activation function. The value function network is composed of two hidden layers of size $64$ and $64$ respectively, with a \ac{ReLU} activation function. The output of the policy network is a vector of size $23$. The output of the value function network is a scalar, which is then used to compute the advantage function.

The \ac{RL} framework has been implemented using the \texttt{Stable Baselines 3} library \citep{raffin_stable-baselines3_2021}. The \ac{RL} framework has been trained for $10$M steps, which given the episodic length, corresponds to $10$ epochs. The INSERT FIGURE below represents the main metrics collected in the training process. In particular, the figure shows the mean reward, the mean episode length, the mean episodic reward, the approximated KL divergence, and the explained variance.

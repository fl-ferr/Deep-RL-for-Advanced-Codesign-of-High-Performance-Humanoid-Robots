
@article{lara_embodied_2018,
	title = {Embodied {Cognitive} {Robotics} and the learning of sensorimotor schemes},
	volume = {26},
	issn = {1059-7123, 1741-2633},
	url = {http://journals.sagepub.com/doi/10.1177/1059712318780679},
	doi = {10.1177/1059712318780679},
	abstract = {Embodied Cognitive Robotics focuses its attention on the design of artificial agents capable of performing cognitive tasks autonomously. A central issue in this consists in studying process by which agents learn through interaction with their environment. Embodied Cognitive Robotics aims to implement models of cognitive processes coming from Cognitive Sciences. The guidelines in this research area are a direct response to the shortcomings of Classical Artificial Intelligence, where high-level tasks and behaviors were studied. This article describes the work carried out in the Cognitive Robotics Laboratory at the Universidad Autónoma del Estado de Morelos (UAEM). Our work is based on the concept of low-level sensorimotor schemes coded by Internal Models, thus falling as a matter of course within the tenets of Embodied Cognition, particularly with the idea that cognition must be understood as occurring in agents that have a body with which they interact in a specific environment. It is through this interaction that learning emerges laying the ground for cognitive processes. Our research includes theoretical work laying the foundations of Embodied Cognitive Robotics, as well as work with artificial and with natural agents.},
	language = {en},
	number = {5},
	urldate = {2023-11-08},
	journal = {Adaptive Behavior},
	author = {Lara, Bruno and Astorga, Dadai and Mendoza-Bock, Emmanuel and Pardo, Manuel and Escobar, Esaú and Ciria, Alejandra},
	month = oct,
	year = {2018},
	pages = {225--238},
}

@article{article,
	title = {A review of control architectures for autonomous navigation of mobile robots},
	volume = {6},
	journal = {International Journal of Physical Sciences},
	author = {Nakhaeinia, Danial and Tang, Sai Hong and Noor, Samsul and Motlagh, O.},
	month = jan,
	year = {2011},
	pages = {169--174},
}

@article{zeng_robotic_2022,
	title = {Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching},
	volume = {41},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364919868017},
	doi = {10.1177/0278364919868017},
	abstract = {This article presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses an object-agnostic grasping framework to map from visual observations to actions: inferring dense pixel-wise probability maps of the affordances for four different grasping primitive actions. It then executes the action with the highest affordance and recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional data collection or re-training. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT–Princeton Team system that took first place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu/},
	language = {en},
	number = {7},
	urldate = {2023-11-08},
	journal = {The International Journal of Robotics Research},
	author = {Zeng, Andy and Song, Shuran and Yu, Kuan-Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and Chavan Dafle, Nikhil and Holladay, Rachel and Morona, Isabella and Nair, Prem Qu and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
	month = jun,
	year = {2022},
	pages = {690--705},
}

@misc{warp2022,
	title = {Warp: {A} high-performance python framework for {GPU} simulation and graphics},
	url = {https://github.com/nvidia/warp},
	author = {Macklin, Miles},
	month = mar,
	year = {2022},
}

@inproceedings{Koenig2004,
	title = {Design and use paradigms for {Gazebo}, an open-source multi-robot simulator},
	volume = {3},
	booktitle = {{IEEE}/{RSJ} international conference on intelligent robots and systems ({IROS}) ({IEEE} cat. {No}.{04CH37566})},
	author = {Koenig, N. and Howard, A.},
	year = {2004},
	note = {tex.added-at: 2020-05-24T20:36:48.000+0200
tex.interhash: 91dd5e35de0ae769be8bbde8ce2e2c4c
tex.intrahash: e7fb6723d0fa2967afb631d9112470c5
tex.timestamp: 2020-05-24T20:36:48.000+0200},
	keywords = {sea},
	pages = {2149--2154},
}

@misc{ode:2008,
	title = {Open dynamics engine},
	url = {http://www.ode.org/},
	author = {Smith, Russell},
	year = {2008},
	note = {tex.added-at: 2009-06-26T15:25:19.000+0200
tex.description: diverse cognitive systems bib
tex.interhash: 8d36e238881c3704e2fb429a63cc8638
tex.intrahash: 55a15eaa9b345a9d1071f5184b324b05
tex.owner: butz
tex.timestamp: 2009-06-26T15:25:56.000+0200},
	keywords = {imported},
}

@inproceedings{erez_simulation_2015,
	address = {Seattle, WA, USA},
	title = {Simulation tools for model-based robotics: {Comparison} of {Bullet}, {Havok}, {MuJoCo}, {ODE} and {PhysX}},
	isbn = {978-1-4799-6923-4},
	shorttitle = {Simulation tools for model-based robotics},
	url = {http://ieeexplore.ieee.org/document/7139807/},
	doi = {10.1109/ICRA.2015.7139807},
	urldate = {2023-11-07},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Erez, Tom and Tassa, Yuval and Todorov, Emanuel},
	month = may,
	year = {2015},
	pages = {4397--4404},
}

@inproceedings{ivaldi:hal-01116148,
	address = {Madrid, Spain},
	title = {Tools for simulating humanoid robot dynamics: a survey based on user feedback},
	url = {https://hal.science/hal-01116148},
	booktitle = {{IEEE}-{RAS} international conference on humanoid robots (humanoids)},
	author = {Ivaldi, Serena and Peters, Jan and Padois, Vincent and Nori, Francesco},
	year = {2014},
	note = {tex.hal\_id: hal-01116148
tex.hal\_version: v1},
}

@misc{drake,
	title = {Drake: {A} planning, control, and analysis toolbox for nonlinear dynamical systems},
	url = {http://drake.mit.edu},
	author = {Tedrake, Russ and Team, the Drake Development},
	year = {2016},
	note = {tex.added-at: 2017-08-22T15:34:32.000+0200
tex.interhash: cf43beb6a04415fe6a83acbfd7087679
tex.intrahash: fa5ba14bf8f8ea50db52df64bac6f184
tex.timestamp: 2017-09-01T13:58:45.000+0200},
	keywords = {c++ cpp dynamics kinematics library robotics},
}

@article{10.3389/frobt.2015.00006,
	title = {{iCub} whole-body control through force regulation on rigid noncoplanar contacts},
	volume = {2},
	issn = {2296-9144},
	url = {http://www.frontiersin.org/humanoid_robotics/10.3389/frobt.2015.00006/abstract},
	doi = {10.3389/frobt.2015.00006},
	number = {6},
	journal = {Frontiers in Robotics and AI},
	author = {Nori, Francesco and Traversaro, Silvio and Eljaik, Jorhabib and Romano, Francesco and Del Prete, Andrea and Pucci, Daniele},
	year = {2015},
}

@article{chaumont_evolution_2016,
	title = {Evolution of sustained foraging in three-dimensional environments with physics},
	volume = {17},
	issn = {1389-2576, 1573-7632},
	url = {http://link.springer.com/10.1007/s10710-016-9270-z},
	doi = {10.1007/s10710-016-9270-z},
	language = {en},
	number = {4},
	urldate = {2023-11-07},
	journal = {Genetic Programming and Evolvable Machines},
	author = {Chaumont, Nicolas and Adami, Christoph},
	month = dec,
	year = {2016},
	pages = {359--390},
}

@article{fadini_simulation_2022,
	title = {Simulation {Aided} {Co}-{Design} for {Robust} {Robot} {Optimization}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9863656/},
	doi = {10.1109/LRA.2022.3200142},
	number = {4},
	urldate = {2023-11-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Fadini, Gabriele and Flayols, Thomas and Prete, Andrea Del and Soueres, Philippe},
	month = oct,
	year = {2022},
	pages = {11306--11313},
}

@inproceedings{metta_icub_2008,
	address = {Gaithersburg Maryland},
	title = {The {iCub} humanoid robot: an open platform for research in embodied cognition},
	isbn = {978-1-60558-293-1},
	shorttitle = {The {iCub} humanoid robot},
	url = {https://dl.acm.org/doi/10.1145/1774674.1774683},
	doi = {10.1145/1774674.1774683},
	language = {en},
	urldate = {2023-11-07},
	booktitle = {Proceedings of the 8th {Workshop} on {Performance} {Metrics} for {Intelligent} {Systems}},
	publisher = {ACM},
	author = {Metta, Giorgio and Sandini, Giulio and Vernon, David and Natale, Lorenzo and Nori, Francesco},
	month = aug,
	year = {2008},
	pages = {50--56},
}

@article{peng_deepmimic_2018,
	title = {{DeepMimic}: example-guided deep reinforcement learning of physics-based character skills},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{DeepMimic}},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201311},
	doi = {10.1145/3197517.3201311},
	abstract = {A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
	language = {en},
	number = {4},
	urldate = {2023-11-07},
	journal = {ACM Transactions on Graphics},
	author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van De Panne, Michiel},
	month = aug,
	year = {2018},
	pages = {1--14},
}

@article{vaisi_review_2022,
	title = {A review of optimization models and applications in robotic manufacturing systems: {Industry} 4.0 and beyond},
	volume = {2},
	issn = {27726622},
	shorttitle = {A review of optimization models and applications in robotic manufacturing systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2772662222000054},
	doi = {10.1016/j.dajour.2022.100031},
	language = {en},
	urldate = {2023-11-07},
	journal = {Decision Analytics Journal},
	author = {Vaisi, Bahareh},
	month = mar,
	year = {2022},
	pages = {100031},
}

@misc{sartore_optimization_2022,
	title = {Optimization of {Humanoid} {Robot} {Designs} for {Human}-{Robot} {Ergonomic} {Payload} {Lifting}},
	url = {http://arxiv.org/abs/2211.13503},
	abstract = {When a human and a humanoid robot collaborate physically, ergonomics is a key factor to consider. Assuming a given humanoid robot, several control architectures exist nowadays to address ergonomic physical human-robot collaboration. This paper takes one step further by considering robot hardware parameters as optimization variables in the problem of collaborative payload lifting. The variables that parametrize robot's kinematics and dynamics ensure their physical consistency, and the human model is considered in the optimization problem. By leveraging the proposed modelling framework, the ergonomy of the interaction is maximized, here given by the agents' energy expenditure. Robot kinematic, dynamics, hardware constraints and human geometries are considered when solving the associated optimization problem. The proposed methodology is used to identify optimum hardware parameters for the design of the ergoCub robot, a humanoid possessing a degree of embodied intelligence for ergonomic interaction with humans. For the optimization problem, the starting point is the iCub humanoid robot. The obtained robot design reaches loads at heights in the range of 0.8-1.5 m with respect to the iCub robot whose range is limited to 0.8-1.2 m. The robot energy expenditure is decreased by about 33\%, meanwhile, the human ergonomy is preserved, leading overall to an improved interaction.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Sartore, Carlotta and Rapetti, Lorenzo and Pucci, Daniele},
	month = nov,
	year = {2022},
	note = {arXiv:2211.13503 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{golroudbari_recent_2023,
	title = {Recent {Advancements} in {Deep} {Learning} {Applications} and {Methods} for {Autonomous} {Navigation}: {A} {Comprehensive} {Review}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Recent {Advancements} in {Deep} {Learning} {Applications} and {Methods} for {Autonomous} {Navigation}},
	url = {https://arxiv.org/abs/2302.11089},
	doi = {10.48550/ARXIV.2302.11089},
	abstract = {This review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.},
	urldate = {2023-11-07},
	author = {Golroudbari, Arman Asgharpoor and Sabour, Mohammad Hossein},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Robotics (cs.RO), Signal Processing (eess.SP), Systems and Control (eess.SY)},
}

@misc{freeman_brax_2021,
	title = {Brax -- {A} {Differentiable} {Physics} {Engine} for {Large} {Scale} {Rigid} {Body} {Simulation}},
	url = {http://arxiv.org/abs/2106.13281},
	abstract = {We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
	month = jun,
	year = {2021},
	note = {arXiv:2106.13281 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@phdthesis{traversaro_modelling_2019,
	type = {phd},
	title = {Modelling, estimation and identification of humanoid robots dynamics},
	url = {https://traversaro.github.io/traversaro-phd-thesis/traversaro-phd-thesis.pdf},
	school = {University of Genoa},
	author = {Traversaro, Silvio},
	month = dec,
	year = {2019},
}

@inproceedings{vassilevska-williams2012breaking,
	title = {Breaking the {Coppersmith}–{Winograd} barrier},
	booktitle = {Proceedings of the 44th {ACM} symposium on theory of computing ({STOC} 2012)},
	author = {Vassilevska-Williams, Virginia},
	year = {2012},
	pages = {887--898},
}

@article{coppersmith_matrix_1990,
	title = {Matrix multiplication via arithmetic progressions},
	volume = {9},
	issn = {07477171},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747717108800132},
	doi = {10.1016/S0747-7171(08)80013-2},
	language = {en},
	number = {3},
	urldate = {2023-10-31},
	journal = {Journal of Symbolic Computation},
	author = {Coppersmith, Don and Winograd, Shmuel},
	month = mar,
	year = {1990},
	pages = {251--280},
}

@article{article,
	title = {Breaking the coppersmith-winograd barrier},
	author = {Williams, Virginia},
	month = sep,
	year = {2014},
}

@book{SicilianoKhatib2008,
	address = {Berlin, Heidelberg},
	title = {Springer handbook of robotics},
	isbn = {978-3-540-23957-4},
	url = {http://dx.doi.org/10.1007/978-3-540-30301-5},
	abstract = {Robotics is undergoing a major transformation in scope and dimension. Starting from a predominantly industrial focus, robotics has been rapidly expanding into the challenges of unstructured environments. The Springer Handbook of Robotics incorporates these new developments and therefore basically differs from other handbooks of robotics focusing on industrial applications. It presents a widespread and well-structured coverage from the foundations of robotics, through the consolidated methodologies and technologies, up to the new emerging application areas of robotics. The handbook is an ideal resource for robotics experts but also for people new to this expanding field such as engineers, medical doctors, computer scientists, designers. Reaching for the human frontier, robotics is vigorously engaged in the growing challenges of new emerging domains. Interacting, exploring, and working with humans, the new generation of robots will increasingly touch people and their lives. The credible prospect of practical robots among humans is the result of the scientific endeavour of a half a century of robotic developments that established robotics as a modern scientific discipline. The Springer Handbook of Robotics brings a widespreadand well-structured compilation of classic and emerging application areas of robotics. From the foundations to the social and ethical implications of robotics, the handbook provides a comprehensive collection of the accomplishments in the field, and constitutes a premise of further advances towards new challenges in robotics. This handbook, edited by two internationally renowned scientists with the support of an outstanding team of seven part editors and onehundred sixty-four authors, is an authoritative reference for robotics researchers, newcomers to the field, and scholars from related disciplines such as biomechanics, neurosciences, virtual simulation, animation, surgery, and sensor networks among others.},
	publisher = {Springer},
	editor = {Siciliano, Bruno and Khatib, Oussama},
	year = {2008},
	note = {tex.added-at: 2008-08-03T03:31:48.000+0200
tex.interhash: d4cb56df9cb34909467d54dc2554f1aa
tex.intrahash: eab99b335cdb05c4d1636008a91bc1c2
tex.owner: flint
tex.timestamp: 2008-08-03T03:31:49.000+0200},
	keywords = {Science robotics springer},
}

@misc{rl-games2021,
	title = {rl-games: {A} high-performance framework for reinforcement learning},
	url = {https://github.com/Denys88/rl_games},
	publisher = {GitHub},
	author = {Makoviichuk, Denys and Makoviychuk, Viktor},
	month = may,
	year = {2021},
}

@misc{zhou_towards_2023,
	title = {Towards {Building} {AI}-{CPS} with {NVIDIA} {Isaac} {Sim}: {An} {Industrial} {Benchmark} and {Case} {Study} for {Robotics} {Manipulation}},
	shorttitle = {Towards {Building} {AI}-{CPS} with {NVIDIA} {Isaac} {Sim}},
	url = {http://arxiv.org/abs/2308.00055},
	abstract = {As a representative cyber-physical system (CPS), robotic manipulator has been widely adopted in various academic research and industrial processes, indicating its potential to act as a universal interface between the cyber and the physical worlds. Recent studies in robotics manipulation have started employing artificial intelligence (AI) approaches as controllers to achieve better adaptability and performance. However, the inherent challenge of explaining AI components introduces uncertainty and unreliability to these AI-enabled robotics systems, necessitating a reliable development platform for system design and performance assessment. As a foundational step towards building reliable AI-enabled robotics systems, we propose a public industrial benchmark for robotics manipulation in this paper. It leverages NVIDIA Omniverse Isaac Sim as the simulation platform, encompassing eight representative manipulation tasks and multiple AI software controllers. An extensive evaluation is conducted to analyze the performance of AI controllers in solving robotics manipulation tasks, enabling a thorough understanding of their effectiveness. To further demonstrate the applicability of our benchmark, we develop a falsification framework that is compatible with physical simulators and OpenAI Gym environments. This framework bridges the gap between traditional testing methods and modern physics engine-based simulations. The effectiveness of different optimization methods in falsifying AI-enabled robotics manipulation with physical simulators is examined via a falsification test. Our work not only establishes a foundation for the design and development of AI-enabled robotics systems but also provides practical experience and guidance to practitioners in this field, promoting further research in this critical academic and industrial domain.},
	urldate = {2023-10-27},
	publisher = {arXiv},
	author = {Zhou, Zhehua and Song, Jiayang and Xie, Xuan and Shu, Zhan and Ma, Lei and Liu, Dikai and Yin, Jianxiong and See, Simon},
	month = jul,
	year = {2023},
	note = {arXiv:2308.00055 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
}

@article{peng_amp_2021,
	title = {{AMP}: {Adversarial} {Motion} {Priors} for {Stylized} {Physics}-{Based} {Character} {Control}},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{AMP}},
	url = {http://arxiv.org/abs/2104.02180},
	doi = {10.1145/3450626.3459670},
	abstract = {Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character's behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.},
	number = {4},
	urldate = {2023-10-27},
	journal = {ACM Transactions on Graphics},
	author = {Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo},
	month = aug,
	year = {2021},
	note = {arXiv:2104.02180 [cs]},
	keywords = {Computer Science - Graphics, Computer Science - Machine Learning},
	pages = {1--20},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2023-10-27},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}

@inproceedings{47008,
	title = {Compiling machine learning programs via high-level tracing},
	url = {https://mlsys.org/Conferences/doc/2018/146.pdf},
	author = {Frostig, Roy and Johnson, Matthew and Leary, Chris},
	year = {2018},
}

@misc{tensorflow2015-whitepaper,
	title = {{TensorFlow}: {Large}-scale machine learning on heterogeneous systems},
	url = {https://www.tensorflow.org/},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2015},
}

@inproceedings{wang_kernel_2010,
	address = {Hangzhou, China},
	title = {Kernel {Fusion}: {An} {Effective} {Method} for {Better} {Power} {Efficiency} on {Multithreaded} {GPU}},
	isbn = {978-1-4244-9779-9},
	shorttitle = {Kernel {Fusion}},
	url = {http://ieeexplore.ieee.org/document/5724850/},
	doi = {10.1109/GreenCom-CPSCom.2010.102},
	urldate = {2023-10-27},
	booktitle = {2010 {IEEE}/{ACM} {Int}'l {Conference} on {Green} {Computing} and {Communications} \& {Int}'l {Conference} on {Cyber}, {Physical} and {Social} {Computing}},
	publisher = {IEEE},
	author = {Wang, Guibin and Lin, YiSong and Yi, Wei},
	month = dec,
	year = {2010},
	pages = {344--350},
}

@misc{snider_operator_2023,
	title = {Operator {Fusion} in {XLA}: {Analysis} and {Evaluation}},
	shorttitle = {Operator {Fusion} in {XLA}},
	url = {http://arxiv.org/abs/2301.13062},
	abstract = {Machine learning (ML) compilers are an active area of research because they offer the potential to automatically speedup tensor programs. Kernel fusion is often cited as an important optimization performed by ML compilers. However, there exists a knowledge gap about how XLA, the most common ML compiler, applies this nuanced optimization, what kind of speedup it can afford, and what low-level effects it has on hardware. Our paper aims to bridge this knowledge gap by studying key compiler passes of XLA's source code. Our evaluation on a reinforcement learning environment Cartpole shows how different fusion decisions in XLA are made in practice. Furthermore, we implement several XLA kernel fusion strategies that can achieve up to 10.56x speedup compared to our baseline implementation.},
	urldate = {2023-10-27},
	publisher = {arXiv},
	author = {Snider, Daniel and Liang, Ruofan},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13062 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{50530,
	title = {{XLA} : {Compiling} machine learning for peak performance},
	author = {Sabne, Amit},
	year = {2020},
}

@misc{suh_differentiable_2022,
	title = {Do {Differentiable} {Simulators} {Give} {Better} {Policy} {Gradients}?},
	url = {http://arxiv.org/abs/2202.00817},
	abstract = {Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an \${\textbackslash}alpha\$-order gradient estimator, with \${\textbackslash}alpha {\textbackslash}in [0,1]\$, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the \${\textbackslash}alpha\$-order estimator on some numerical examples.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Suh, H. J. Terry and Simchowitz, Max and Zhang, Kaiqing and Tedrake, Russ},
	month = aug,
	year = {2022},
	note = {arXiv:2202.00817 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@book{noauthor_notitle_nodate,
}

@article{sciavicco_lagrange_1995,
	title = {Lagrange and {Newton}-{Euler} dynamic modeling of a gear-driven robot manipulator with inclusion of motor inertia effects},
	volume = {10},
	issn = {0169-1864, 1568-5535},
	url = {https://www.tandfonline.com/doi/full/10.1163/156855395X00427},
	doi = {10.1163/156855395X00427},
	language = {en},
	number = {3},
	urldate = {2023-08-30},
	journal = {Advanced Robotics},
	author = {Sciavicco, Lorenzo and Siciliano, Bruno and Villani, Luigi},
	month = jan,
	year = {1995},
	pages = {317--334},
}

@misc{agarwal_deep_2022,
	title = {Deep {Reinforcement} {Learning} at the {Edge} of the {Statistical} {Precipice}},
	url = {http://arxiv.org/abs/2108.13264},
	abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
	month = jan,
	year = {2022},
	note = {arXiv:2108.13264 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{akiba_optuna_2019,
	title = {Optuna: {A} {Next}-generation {Hyperparameter} {Optimization} {Framework}},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	year = {2019},
}

@article{raffin_stable-baselines3_2021,
	title = {Stable-{Baselines3}: {Reliable} {Reinforcement} {Learning} {Implementations}},
	volume = {22},
	url = {http://jmlr.org/papers/v22/20-1364.html},
	number = {268},
	journal = {Journal of Machine Learning Research},
	author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
	year = {2021},
	pages = {1--8},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{todorov_mujoco_2012,
	address = {Vilamoura-Algarve, Portugal},
	title = {{MuJoCo}: {A} physics engine for model-based control},
	isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
	shorttitle = {{MuJoCo}},
	url = {http://ieeexplore.ieee.org/document/6386109/},
	doi = {10.1109/IROS.2012.6386109},
	urldate = {2023-08-08},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	month = oct,
	year = {2012},
	pages = {5026--5033},
}

@misc{coumans_pybullet_2016,
	title = {{PyBullet}, a {Python} module for physics simulation for games, robotics and machine learning},
	url = {http://pybullet.org},
	author = {Coumans, Erwin and Bai, Yunfei},
	year = {2016},
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	language = {en},
	number = {3-4},
	urldate = {2023-08-07},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	pages = {229--256},
}

@incollection{daelemans_state-dependent_2008,
	address = {Berlin, Heidelberg},
	title = {State-{Dependent} {Exploration} for {Policy} {Gradient} {Methods}},
	volume = {5212},
	isbn = {978-3-540-87480-5 978-3-540-87481-2},
	url = {http://link.springer.com/10.1007/978-3-540-87481-2_16},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rückstieß, Thomas and Felder, Martin and Schmidhuber, Jürgen},
	editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
	year = {2008},
	doi = {10.1007/978-3-540-87481-2_16},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {234--249},
}

@misc{makoviychuk_isaac_2021,
	title = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
	shorttitle = {Isaac {Gym}},
	url = {http://arxiv.org/abs/2108.10470},
	abstract = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at {\textbackslash}url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at {\textbackslash}url\{https://developer.nvidia.com/isaac-gym\}.},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
	month = aug,
	year = {2021},
	note = {arXiv:2108.10470 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv:1506.02438 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2023-08-03},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{deb_evolutionary_2014,
	title = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}: {Solving} {Problems} {With} {Box} {Constraints}},
	volume = {18},
	issn = {1089-778X, 1089-778X, 1941-0026},
	shorttitle = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}},
	url = {http://ieeexplore.ieee.org/document/6600851/},
	doi = {10.1109/TEVC.2013.2281535},
	number = {4},
	urldate = {2023-08-02},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Deb, Kalyanmoy and Jain, Himanshu},
	month = aug,
	year = {2014},
	pages = {577--601},
}

@article{gu_modified_2021,
	title = {Modified non-dominated sorting genetic algorithm {III} with fine final level selection},
	volume = {51},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-020-02053-z},
	doi = {10.1007/s10489-020-02053-z},
	language = {en},
	number = {7},
	urldate = {2023-08-02},
	journal = {Applied Intelligence},
	author = {Gu, Qinghua and Wang, Rui and Xie, Haiyan and Li, Xuexian and Jiang, Song and Xiong, Naixue},
	month = jul,
	year = {2021},
	pages = {4236--4269},
}

@misc{raffin_smooth_2021,
	title = {Smooth {Exploration} for {Robotic} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.05719},
	abstract = {Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {Raffin, Antonin and Kober, Jens and Stulp, Freek},
	month = jun,
	year = {2021},
	note = {arXiv:2005.05719 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@book{featherstone_rigid_2008,
	address = {Boston, MA},
	title = {Rigid {Body} {Dynamics} {Algorithms}},
	isbn = {978-0-387-74314-1},
	url = {http://link.springer.com/10.1007/978-1-4899-7560-7},
	language = {en},
	urldate = {2023-07-28},
	publisher = {Springer US},
	author = {Featherstone, Roy},
	year = {2008},
	doi = {10.1007/978-1-4899-7560-7},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{makoviychuk_isaac_2021-1,
	title = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
	author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
	year = {2021},
	note = {Publication Title: arXiv preprint arXiv:2108.10470},
}

@misc{liang_gpu-accelerated_2018,
	title = {{GPU}-{Accelerated} {Robotic} {Simulation} for {Distributed} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1810.05762},
	abstract = {Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Liang, Jacky and Makoviychuk, Viktor and Handa, Ankur and Chentanez, Nuttapong and Macklin, Miles and Fox, Dieter},
	month = oct,
	year = {2018},
	note = {arXiv:1810.05762 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv:1606.01540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: composable transformations of {Python}+{NumPy} programs},
	url = {http://github.com/google/jax},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
	year = {2018},
}

@misc{ferigo_jaxsim_2022,
	title = {{JAXsim}: {A} {Physics} {Engine} in {Reduced} {Coordinates} for {Control} and {Robot} {Learning}},
	url = {http://github.com/ami-iit/jaxsin},
	author = {Ferigo, Diego and Traversaro, Silvio and Pucci, Daniele},
	year = {2022},
}

@misc{heek_flax_2023,
	title = {Flax: {A} neural network library and ecosystem for {JAX}},
	url = {http://github.com/google/flax},
	author = {Heek, Jonathan and Levskaya, Anselm and Oliver, Avital and Ritter, Marvin and Rondepierre, Bertrand and Steiner, Andreas and Zee, Marc van},
	year = {2023},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Connectors}},
	url = {https://www.zotero.org/download/connectors},
	urldate = {2023-06-15},
}

@misc{noauthor_zotero_nodate-1,
	title = {Zotero {\textbar} {Connectors}},
	url = {https://www.zotero.org/download/connectors},
	urldate = {2023-06-15},
}

@misc{li_deep_2018,
	title = {Deep {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1810.06339},
	abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Li, Yuxi},
	month = oct,
	year = {2018},
	note = {arXiv:1810.06339 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_deep_2018-1,
	title = {Deep {Reinforcement} {Learning}: {An} {Overview}},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1701.07274},
	abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Li, Yuxi},
	month = nov,
	year = {2018},
	note = {arXiv:1701.07274 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_reinforcement_2021,
	title = {Reinforcement {Learning} for {Robust} {Parameterized} {Locomotion} {Control} of {Bipedal} {Robots}},
	url = {http://arxiv.org/abs/2103.14295},
	abstract = {Developing robust walking controllers for bipedal robots is a challenging endeavor. Traditional model-based locomotion controllers require simplifying assumptions and careful modelling; any small errors can result in unstable control. To address these challenges for bipedal locomotion, we present a model-free reinforcement learning framework for training robust locomotion policies in simulation, which can then be transferred to a real bipedal Cassie robot. To facilitate sim-to-real transfer, domain randomization is used to encourage the policies to learn behaviors that are robust across variations in system dynamics. The learned policies enable Cassie to perform a set of diverse and dynamic behaviors, while also being more robust than traditional controllers and prior learning-based methods that use residual control. We demonstrate this on versatile walking behaviors such as tracking a target walking velocity, walking height, and turning yaw.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Li, Zhongyu and Cheng, Xuxin and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14295 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{featherstone_calculation_1983,
	title = {The {Calculation} of {Robot} {Dynamics} {Using} {Articulated}-{Body} {Inertias}},
	volume = {2},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/027836498300200102},
	doi = {10.1177/027836498300200102},
	abstract = {This paper describes a new method for calculating the acceleration of a robot in response to given actuator forces. The method is applicable to open-loop kinematic chains containing revolute and prismatic joints. The algorithm is based on recursive formulas involving quantities called articulated-body inertias, which represent the inertia properties of collections of rigid bodies connected together by joints allowing constrained relative motion between the bodies. A new, matrix-based notation is introduced to represent articulated-body inertias and other spatial quantities. This notation is used to develop the algorithm, and results in a compact representation of the equations. The new algorithm has a computational requirement that varies linearly with the number of joints, and its efficiency is compared with other published algorithms.},
	language = {en},
	number = {1},
	urldate = {2023-06-13},
	journal = {The International Journal of Robotics Research},
	author = {Featherstone, R.},
	month = mar,
	year = {1983},
	pages = {13--30},
}

@misc{chi_diffusion_2023,
	title = {Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion}},
	shorttitle = {Diffusion {Policy}},
	url = {http://arxiv.org/abs/2303.04137},
	abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
	month = jun,
	year = {2023},
	note = {arXiv:2303.04137 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{farsang_decaying_2021,
	title = {Decaying {Clipping} {Range} in {Proximal} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/2102.10456},
	doi = {10.1109/SACI51354.2021.9465602},
	abstract = {Proximal Policy Optimization (PPO) is among the most widely used algorithms in reinforcement learning, which achieves state-of-the-art performance in many challenging problems. The keys to its success are the reliable policy updates through the clipping mechanism and the multiple epochs of minibatch updates. The aim of this research is to give new simple but effective alternatives to the former. For this, we propose linearly and exponentially decaying clipping range approaches throughout the training. With these, we would like to provide higher exploration at the beginning and stronger restrictions at the end of the learning phase. We investigate their performance in several classical control and locomotive robotic environments. During the analysis, we found that they influence the achieved rewards and are effective alternatives to the constant clipping method in many reinforcement learning tasks.},
	urldate = {2023-06-05},
	booktitle = {2021 {IEEE} 15th {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics} ({SACI})},
	author = {Farsang, Mónika and Szegletes, Luca},
	month = may,
	year = {2021},
	note = {arXiv:2102.10456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	pages = {000521--000526},
}

@misc{nakka_trajectory_2022,
	title = {Trajectory {Optimization} of {Chance}-{Constrained} {Nonlinear} {Stochastic} {Systems} for {Motion} {Planning} {Under} {Uncertainty}},
	url = {http://arxiv.org/abs/2106.02801},
	abstract = {We present gPC-SCP: Generalized Polynomial Chaos-based Sequential Convex Programming to compute a sub-optimal solution for a continuous-time chance-constrained stochastic nonlinear optimal control (SNOC) problem. The approach enables motion planning for robotic systems under uncertainty. The gPC-SCP method involves two steps. The first step is to derive a surrogate problem of {\textbackslash}emph\{deterministic\} nonlinear optimal control (DNOC) with convex constraints by using gPC expansion and the distributionally-robust convex subset of the chance constraints. The second step is to solve the DNOC problem using sequential convex programming for trajectory generation and control. We prove that in the unconstrained case, the optimal value of the DNOC converges to that of SNOC asymptotically and that any feasible solution of the constrained DNOC is a feasible solution of the chance-constrained SNOC. We also present the predictor-corrector extension (gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$) for real-time motion trajectory generation in the presence of stochastic uncertainty. In the gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$ method, we first predict the uncertainty using the gPC method and then optimize the motion plan to accommodate the uncertainty. We empirically demonstrate the efficacy of the gPC-SCP and the gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$ methods for the following two test cases: 1) collision checking under uncertainty in actuation and physical parameters and 2) collision checking with stochastic obstacle model for 3DOF and 6DOF robotic systems. We validate the effectiveness of the gPC-SCP method on the 3DOF robotic spacecraft testbed.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Nakka, Yashwanth Kumar and Chung, Soon-Jo},
	month = mar,
	year = {2022},
	note = {arXiv:2106.02801 [cs, eess, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@book{puterman_markov_1994,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	isbn = {978-0-471-61977-2 978-0-470-31688-7},
	shorttitle = {Markov {Decision} {Processes}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
	language = {en},
	urldate = {2023-05-26},
	publisher = {Wiley},
	author = {Puterman, Martin L.},
	month = apr,
	year = {1994},
	doi = {10.1002/9780470316887},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{pardo_time_2022,
	title = {Time {Limits} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1712.00378},
	abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
	month = jan,
	year = {2022},
	note = {arXiv:1712.00378 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{bjelonic_learning-based_2023,
	title = {Learning-based {Design} and {Control} for {Quadrupedal} {Robots} with {Parallel}-{Elastic} {Actuators}},
	volume = {8},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2301.03509},
	doi = {10.1109/LRA.2023.3234809},
	abstract = {Parallel-elastic joints can improve the efficiency and strength of robots by assisting the actuators with additional torques. For these benefits to be realized, a spring needs to be carefully designed. However, designing robots is an iterative and tedious process, often relying on intuition and heuristics. We introduce a design optimization framework that allows us to co-optimize a parallel elastic knee joint and locomotion controller for quadrupedal robots with minimal human intuition. We design a parallel elastic joint and optimize its parameters with respect to the efficiency in a model-free fashion. In the first step, we train a design-conditioned policy using model-free Reinforcement Learning, capable of controlling the quadruped in the predefined range of design parameters. Afterwards, we use Bayesian Optimization to find the best design using the policy. We use this framework to optimize the parallel-elastic spring parameters for the knee of our quadrupedal robot ANYmal together with the optimal controller. We evaluate the optimized design and controller in real-world experiments over various terrains. Our results show that the new system improves the torque-square efficiency of the robot by 33\% compared to the baseline and reduces maximum joint torque by 30\% without compromising tracking performance. The improved design resulted in 11\% longer operation time on flat terrain.},
	number = {3},
	urldate = {2023-05-25},
	journal = {IEEE Robotics and Automation Letters},
	author = {Bjelonic, Filip and Lee, Joonho and Arm, Philip and Sako, Dhionis and Tateo, Davide and Peters, Jan and Hutter, Marco},
	month = mar,
	year = {2023},
	note = {arXiv:2301.03509 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1611--1618},
}

@article{song_policy_2022,
	title = {Policy {Search} for {Model} {Predictive} {Control} {With} {Application} to {Agile} {Drone} {Flight}},
	volume = {38},
	issn = {1552-3098, 1941-0468},
	url = {https://ieeexplore.ieee.org/document/9719129/},
	doi = {10.1109/TRO.2022.3141602},
	number = {4},
	urldate = {2023-05-25},
	journal = {IEEE Transactions on Robotics},
	author = {Song, Yunlong and Scaramuzza, Davide},
	month = aug,
	year = {2022},
	pages = {2114--2130},
}

@techreport{traversaro_multibody_2019,
	title = {Multibody dynamics notation (version 2)},
	shorttitle = {Multibody dynamics notation},
	url = {https://research.tue.nl/en/publications/multibody-dynamics-notation-version-2},
	abstract = {This document provides a revision of the notation originally introduced in [20] for describing kinematics and dynamics quantities of mechanical systems composed by several rigid bodies. Relative to the first edition, this new version includes an expanded section on frame acceleration (Section 5.4), the correction of a few typos, and the change of the fonts used in the notation from single face to bold face.
The notation detailed in this document is inspired by the well-known Featherstone notation introduced in [7], also used, with small adaptations, in the Handbook of Robotics [16]. Featherstone’s notation, while being extremely compact and pleasant for the eye, is not fully in accordance with Lie group formalism, with the potential of creating a misunderstanding between the robotics and geometric mechanics communities.
The Lie group formalism is well established in the robotics literature [13, 14,
10]. However, it is less compact than Featherstone’s notation [7], leading to long expressions when several rigid bodies are present as in the case of a complete dynamic model of humanoid or quadruped robots. This report aims, therefore, at getting the best from these two worlds. The notation strives to be compact, precise, and in harmony with Lie Group formalism. The document furthermore introduces a flexible and unambiguous notation
to describe the Jacobians mapping generalized velocities of an arbitrary frame to Cartesian linear and angular velocities, expressed with respect to a reference frame of choice.},
	language = {English},
	institution = {Technische Universiteit Eindhoven},
	author = {Traversaro, Silvio and Saccon, Alessandro},
	year = {2019},
	keywords = {lie group, multibody dynamics, robot, robotics},
	pages = {22},
}

@article{ha_reinforcement_2019,
	title = {Reinforcement {Learning} for {Improving} {Agent} {Design}},
	volume = {25},
	issn = {1064-5462, 1530-9185},
	url = {http://arxiv.org/abs/1810.03779},
	doi = {10.1162/artl_a_00301},
	abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications. Videos of results at https://designrl.github.io/},
	number = {4},
	urldate = {2023-05-25},
	journal = {Artificial Life},
	author = {Ha, David},
	month = nov,
	year = {2019},
	note = {arXiv:1810.03779 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {352--365},
}

@misc{tobin_domain_2017,
	title = {Domain {Randomization} for {Transferring} {Deep} {Neural} {Networks} from {Simulation} to the {Real} {World}},
	url = {http://arxiv.org/abs/1703.06907},
	abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	month = mar,
	year = {2017},
	note = {arXiv:1703.06907 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{sims_evolving_1994,
	title = {Evolving {3D} {Morphology} and {Behavior} by {Competition}},
	volume = {1},
	issn = {1064-5462, 1530-9185},
	url = {https://direct.mit.edu/artl/article/1/4/353-372/2234},
	doi = {10.1162/artl.1994.1.4.353},
	abstract = {This article describes a system for the evolution and coevolution of virtual creatures that compete in physically simulated three-dimensional worlds. Pairs of individuals enter one-on-one contests in which they contend to gain control of a common resource. The winners receive higher relative fitness scores allowing them to survive and reproduce. Realistic dynamics simulation including gravity, collisions, and friction, restricts the actions to physically plausible behaviors.
            The morphology of these creatures and the neural systems for controlling their muscle forces are both genetically determined, and the morphology and behavior can adapt to each other as they evolve simultaneously. The genotypes are structured as directed graphs of nodes and connections, and they can efficiently but flexibly describe instructions for the development of creatures' bodies and control systems with repeating or recursive components. When simulated evolutions are performed with populations of competing creatures, interesting and diverse strategies and counterstrategies emerge.},
	language = {en},
	number = {4},
	urldate = {2023-05-25},
	journal = {Artificial Life},
	author = {Sims, Karl},
	month = jul,
	year = {1994},
	pages = {353--372},
}

@article{rojas_easy_2022,
	title = {An {Easy} to {Use} {Deep} {Reinforcement} {Learning} {Library} for {AI} {Mobile} {Robots} in {Isaac} {Sim}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/17/8429},
	doi = {10.3390/app12178429},
	abstract = {The use of mobile robots for personal and industrial uses is becoming popular. Currently, many robot simulators with high-graphical capabilities can be used by engineering to develop and test these robots such as Isaac Sim. However, using that simulator to train mobile robots with the deep reinforcement learning paradigm can be very difficult and time-consuming if one wants to develop a custom experiment, requiring an understanding of several libraries and APIs to use them together correctly. The proposed work aims to create a library that conceals configuration problems in creating robots, environments, and training scenarios, reducing the time dedicated to code. Every developed method is equivalent to sixty-five lines of code at maximum and five at minimum. That brings time saving in simulated experiments and data collection, thus reducing the time to produce and test viable algorithms for robots in the industry or academy.},
	language = {en},
	number = {17},
	urldate = {2023-05-25},
	journal = {Applied Sciences},
	author = {Rojas, Maximiliano and Hermosilla, Gabriel and Yunge, Daniel and Farias, Gonzalo},
	month = aug,
	year = {2022},
	pages = {8429},
}

@misc{chen_hardware_2020,
	title = {Hardware as {Policy}: {Mechanical} and {Computational} {Co}-{Optimization} using {Deep} {Reinforcement} {Learning}},
	shorttitle = {Hardware as {Policy}},
	url = {http://arxiv.org/abs/2008.04460},
	abstract = {Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment. In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model the robot body as a "hardware policy", analogous to and optimized jointly with its computational counterpart. We show that, by modeling such hardware policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family. We present two such design examples: a toy mass-spring problem, and a real-world problem of designing an underactuated hand. We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters. Videos and more details are available at https://roamlab.github.io/hwasp/ .},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Chen, Tianjian and He, Zhanpeng and Ciocarlie, Matei},
	month = nov,
	year = {2020},
	note = {arXiv:2008.04460 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{schulman_trust_2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv:1502.05477 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_sample_2017,
	title = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1611.01224},
	abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	month = jul,
	year = {2017},
	note = {arXiv:1611.01224 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@book{sutton_reinforcement_1998,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	keywords = {Reinforcement learning},
}

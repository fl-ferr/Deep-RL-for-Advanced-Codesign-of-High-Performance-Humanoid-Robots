
@inproceedings{akiba_optuna_2019,
	title = {Optuna: {A} {Next}-generation {Hyperparameter} {Optimization} {Framework}},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	year = {2019},
}

@article{raffin_stable-baselines3_2021,
  title   = {Stable-{Baselines3}: {Reliable} {Reinforcement} {Learning} {Implementations}},
  volume  = {22},
  url     = {http://jmlr.org/papers/v22/20-1364.html},
  number  = {268},
  journal = {Journal of Machine Learning Research},
  author  = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
  year    = {2021},
  pages   = {1--8}
}

@misc{kingma_adam_2017,
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate    = {2023-08-08},
  publisher  = {arXiv},
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  month      = jan,
  year       = {2017},
  note       = {arXiv:1412.6980 [cs]},
  keywords   = {Computer Science - Machine Learning}
}

@inproceedings{todorov_mujoco_2012,
  address    = {Vilamoura-Algarve, Portugal},
  title      = {{MuJoCo}: {A} physics engine for model-based control},
  isbn       = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
  shorttitle = {{MuJoCo}},
  url        = {http://ieeexplore.ieee.org/document/6386109/},
  doi        = {10.1109/IROS.2012.6386109},
  urldate    = {2023-08-08},
  booktitle  = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
  publisher  = {IEEE},
  author     = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  month      = oct,
  year       = {2012},
  pages      = {5026--5033}
}

@misc{coumans_pybullet_2016,
  title  = {{PyBullet}, a {Python} module for physics simulation for games, robotics and machine learning},
  url    = {http://pybullet.org},
  author = {Coumans, Erwin and Bai, Yunfei},
  year   = {2016}
}

@article{williams_simple_1992,
  title    = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  volume   = {8},
  issn     = {0885-6125, 1573-0565},
  url      = {http://link.springer.com/10.1007/BF00992696},
  doi      = {10.1007/BF00992696},
  language = {en},
  number   = {3-4},
  urldate  = {2023-08-07},
  journal  = {Machine Learning},
  author   = {Williams, Ronald J.},
  month    = may,
  year     = {1992},
  pages    = {229--256}
}

@incollection{daelemans_state-dependent_2008,
  address   = {Berlin, Heidelberg},
  title     = {State-{Dependent} {Exploration} for {Policy} {Gradient} {Methods}},
  volume    = {5212},
  isbn      = {978-3-540-87480-5 978-3-540-87481-2},
  url       = {http://link.springer.com/10.1007/978-3-540-87481-2_16},
  language  = {en},
  urldate   = {2023-08-07},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Rückstieß, Thomas and Felder, Martin and Schmidhuber, Jürgen},
  editor    = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
  year      = {2008},
  doi       = {10.1007/978-3-540-87481-2_16},
  note      = {ISSN: 0302-9743, 1611-3349
               Series Title: Lecture Notes in Computer Science},
  pages     = {234--249}
}

@misc{makoviychuk_isaac_2021,
  title      = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
  shorttitle = {Isaac {Gym}},
  url        = {http://arxiv.org/abs/2108.10470},
  abstract   = {Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at {\textbackslash}url\{https://sites.google.com/view/isaacgym-nvidia\} and isaac gym can be downloaded at {\textbackslash}url\{https://developer.nvidia.com/isaac-gym\}.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
  month      = aug,
  year       = {2021},
  note       = {arXiv:2108.10470 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Robotics}
}

@misc{schulman_high-dimensional_2018,
  title     = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
  url       = {http://arxiv.org/abs/1506.02438},
  abstract  = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  month     = oct,
  year      = {2018},
  note      = {arXiv:1506.02438 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control}
}

@misc{levine_offline_2020,
  title      = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
  shorttitle = {Offline {Reinforcement} {Learning}},
  url        = {http://arxiv.org/abs/2005.01643},
  abstract   = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  urldate    = {2023-08-03},
  publisher  = {arXiv},
  author     = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  month      = nov,
  year       = {2020},
  note       = {arXiv:2005.01643 [cs, stat]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{deb_evolutionary_2014,
  title      = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}: {Solving} {Problems} {With} {Box} {Constraints}},
  volume     = {18},
  issn       = {1089-778X, 1089-778X, 1941-0026},
  shorttitle = {An {Evolutionary} {Many}-{Objective} {Optimization} {Algorithm} {Using} {Reference}-{Point}-{Based} {Nondominated} {Sorting} {Approach}, {Part} {I}},
  url        = {http://ieeexplore.ieee.org/document/6600851/},
  doi        = {10.1109/TEVC.2013.2281535},
  number     = {4},
  urldate    = {2023-08-02},
  journal    = {IEEE Transactions on Evolutionary Computation},
  author     = {Deb, Kalyanmoy and Jain, Himanshu},
  month      = aug,
  year       = {2014},
  pages      = {577--601}
}

@article{gu_modified_2021,
  title    = {Modified non-dominated sorting genetic algorithm {III} with fine final level selection},
  volume   = {51},
  issn     = {0924-669X, 1573-7497},
  url      = {https://link.springer.com/10.1007/s10489-020-02053-z},
  doi      = {10.1007/s10489-020-02053-z},
  language = {en},
  number   = {7},
  urldate  = {2023-08-02},
  journal  = {Applied Intelligence},
  author   = {Gu, Qinghua and Wang, Rui and Xie, Haiyan and Li, Xuexian and Jiang, Song and Xiong, Naixue},
  month    = jul,
  year     = {2021},
  pages    = {4236--4269}
}

@misc{raffin_smooth_2021,
  title     = {Smooth {Exploration} for {Robotic} {Reinforcement} {Learning}},
  url       = {http://arxiv.org/abs/2005.05719},
  abstract  = {Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.},
  urldate   = {2023-07-28},
  publisher = {arXiv},
  author    = {Raffin, Antonin and Kober, Jens and Stulp, Freek},
  month     = jun,
  year      = {2021},
  note      = {arXiv:2005.05719 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning}
}

@book{featherstone_rigid_2008,
  address   = {Boston, MA},
  title     = {Rigid {Body} {Dynamics} {Algorithms}},
  isbn      = {978-0-387-74314-1},
  url       = {http://link.springer.com/10.1007/978-1-4899-7560-7},
  language  = {en},
  urldate   = {2023-07-28},
  publisher = {Springer US},
  author    = {Featherstone, Roy},
  year      = {2008},
  doi       = {10.1007/978-1-4899-7560-7}
}

@misc{ioffe_batch_2015,
  title      = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
  shorttitle = {Batch {Normalization}},
  url        = {http://arxiv.org/abs/1502.03167},
  abstract   = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  urldate    = {2023-07-19},
  publisher  = {arXiv},
  author     = {Ioffe, Sergey and Szegedy, Christian},
  month      = mar,
  year       = {2015},
  note       = {arXiv:1502.03167 [cs]},
  keywords   = {Computer Science - Machine Learning}
}

@misc{makoviychuk_isaac_2021-1,
  title  = {Isaac {Gym}: {High} {Performance} {GPU}-{Based} {Physics} {Simulation} {For} {Robot} {Learning}},
  author = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and State, Gavriel},
  year   = {2021},
  note   = {Publication Title: arXiv preprint arXiv:2108.10470}
}

@misc{noauthor_notitle_nodate
}

@misc{liang_gpu-accelerated_2018,
  title     = {{GPU}-{Accelerated} {Robotic} {Simulation} for {Distributed} {Reinforcement} {Learning}},
  url       = {http://arxiv.org/abs/1810.05762},
  abstract  = {Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.},
  urldate   = {2023-06-15},
  publisher = {arXiv},
  author    = {Liang, Jacky and Makoviychuk, Viktor and Handa, Ankur and Chentanez, Nuttapong and Macklin, Miles and Fox, Dieter},
  month     = oct,
  year      = {2018},
  note      = {arXiv:1810.05762 [cs]},
  keywords  = {Computer Science - Robotics}
}

@misc{brockman_openai_2016,
  title     = {{OpenAI} {Gym}},
  url       = {http://arxiv.org/abs/1606.01540},
  abstract  = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  urldate   = {2023-06-15},
  publisher = {arXiv},
  author    = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1606.01540 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@misc{bradbury_jax_2018,
  title  = {{JAX}: composable transformations of {Python}+{NumPy} programs},
  url    = {http://github.com/google/jax},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  year   = {2018}
}

@misc{ferigo_jaxsim_2022,
  title  = {{JAXsim}: {A} {Physics} {Engine} in {Reduced} {Coordinates} for {Control} and {Robot} {Learning}},
  url    = {http://github.com/ami-iit/jaxsin},
  author = {Ferigo, Diego and Traversaro, Silvio and Pucci, Daniele},
  year   = {2022}
}

@misc{heek_flax_2023,
  title  = {Flax: {A} neural network library and ecosystem for {JAX}},
  url    = {http://github.com/google/flax},
  author = {Heek, Jonathan and Levskaya, Anselm and Oliver, Avital and Ritter, Marvin and Rondepierre, Bertrand and Steiner, Andreas and Zee, Marc van},
  year   = {2023}
}

@misc{noauthor_notitle_nodate-1
}

@misc{noauthor_zotero_nodate,
  title   = {Zotero {\textbar} {Connectors}},
  url     = {https://www.zotero.org/download/connectors},
  urldate = {2023-06-15}
}

@misc{noauthor_zotero_nodate-1,
  title   = {Zotero {\textbar} {Connectors}},
  url     = {https://www.zotero.org/download/connectors},
  urldate = {2023-06-15}
}

@misc{li_deep_2018,
  title     = {Deep {Reinforcement} {Learning}},
  url       = {http://arxiv.org/abs/1810.06339},
  abstract  = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  urldate   = {2023-06-15},
  publisher = {arXiv},
  author    = {Li, Yuxi},
  month     = oct,
  year      = {2018},
  note      = {arXiv:1810.06339 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{li_deep_2018-1,
  title      = {Deep {Reinforcement} {Learning}: {An} {Overview}},
  shorttitle = {Deep {Reinforcement} {Learning}},
  url        = {http://arxiv.org/abs/1701.07274},
  abstract   = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
  urldate    = {2023-06-15},
  publisher  = {arXiv},
  author     = {Li, Yuxi},
  month      = nov,
  year       = {2018},
  note       = {arXiv:1701.07274 [cs]},
  keywords   = {Computer Science - Machine Learning}
}

@misc{li_reinforcement_2021,
  title     = {Reinforcement {Learning} for {Robust} {Parameterized} {Locomotion} {Control} of {Bipedal} {Robots}},
  url       = {http://arxiv.org/abs/2103.14295},
  abstract  = {Developing robust walking controllers for bipedal robots is a challenging endeavor. Traditional model-based locomotion controllers require simplifying assumptions and careful modelling; any small errors can result in unstable control. To address these challenges for bipedal locomotion, we present a model-free reinforcement learning framework for training robust locomotion policies in simulation, which can then be transferred to a real bipedal Cassie robot. To facilitate sim-to-real transfer, domain randomization is used to encourage the policies to learn behaviors that are robust across variations in system dynamics. The learned policies enable Cassie to perform a set of diverse and dynamic behaviors, while also being more robust than traditional controllers and prior learning-based methods that use residual control. We demonstrate this on versatile walking behaviors such as tracking a target walking velocity, walking height, and turning yaw.},
  urldate   = {2023-06-14},
  publisher = {arXiv},
  author    = {Li, Zhongyu and Cheng, Xuxin and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  month     = mar,
  year      = {2021},
  note      = {arXiv:2103.14295 [cs, eess]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control}
}

@article{featherstone_calculation_1983,
  title    = {The {Calculation} of {Robot} {Dynamics} {Using} {Articulated}-{Body} {Inertias}},
  volume   = {2},
  issn     = {0278-3649, 1741-3176},
  url      = {http://journals.sagepub.com/doi/10.1177/027836498300200102},
  doi      = {10.1177/027836498300200102},
  abstract = {This paper describes a new method for calculating the acceleration of a robot in response to given actuator forces. The method is applicable to open-loop kinematic chains containing revolute and prismatic joints. The algorithm is based on recursive formulas involving quantities called articulated-body inertias, which represent the inertia properties of collections of rigid bodies connected together by joints allowing constrained relative motion between the bodies. A new, matrix-based notation is introduced to represent articulated-body inertias and other spatial quantities. This notation is used to develop the algorithm, and results in a compact representation of the equations. The new algorithm has a computational requirement that varies linearly with the number of joints, and its efficiency is compared with other published algorithms.},
  language = {en},
  number   = {1},
  urldate  = {2023-06-13},
  journal  = {The International Journal of Robotics Research},
  author   = {Featherstone, R.},
  month    = mar,
  year     = {1983},
  pages    = {13--30}
}

@misc{chi_diffusion_2023,
  title      = {Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion}},
  shorttitle = {Diffusion {Policy}},
  url        = {http://arxiv.org/abs/2303.04137},
  abstract   = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
  urldate    = {2023-06-06},
  publisher  = {arXiv},
  author     = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2303.04137 [cs]},
  keywords   = {Computer Science - Robotics}
}

@inproceedings{farsang_decaying_2021,
  title     = {Decaying {Clipping} {Range} in {Proximal} {Policy} {Optimization}},
  url       = {http://arxiv.org/abs/2102.10456},
  doi       = {10.1109/SACI51354.2021.9465602},
  abstract  = {Proximal Policy Optimization (PPO) is among the most widely used algorithms in reinforcement learning, which achieves state-of-the-art performance in many challenging problems. The keys to its success are the reliable policy updates through the clipping mechanism and the multiple epochs of minibatch updates. The aim of this research is to give new simple but effective alternatives to the former. For this, we propose linearly and exponentially decaying clipping range approaches throughout the training. With these, we would like to provide higher exploration at the beginning and stronger restrictions at the end of the learning phase. We investigate their performance in several classical control and locomotive robotic environments. During the analysis, we found that they influence the achieved rewards and are effective alternatives to the constant clipping method in many reinforcement learning tasks.},
  urldate   = {2023-06-05},
  booktitle = {2021 {IEEE} 15th {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics} ({SACI})},
  author    = {Farsang, Mónika and Szegletes, Luca},
  month     = may,
  year      = {2021},
  note      = {arXiv:2102.10456 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
  pages     = {000521--000526}
}

@misc{nakka_trajectory_2022,
  title     = {Trajectory {Optimization} of {Chance}-{Constrained} {Nonlinear} {Stochastic} {Systems} for {Motion} {Planning} {Under} {Uncertainty}},
  url       = {http://arxiv.org/abs/2106.02801},
  abstract  = {We present gPC-SCP: Generalized Polynomial Chaos-based Sequential Convex Programming to compute a sub-optimal solution for a continuous-time chance-constrained stochastic nonlinear optimal control (SNOC) problem. The approach enables motion planning for robotic systems under uncertainty. The gPC-SCP method involves two steps. The first step is to derive a surrogate problem of {\textbackslash}emph\{deterministic\} nonlinear optimal control (DNOC) with convex constraints by using gPC expansion and the distributionally-robust convex subset of the chance constraints. The second step is to solve the DNOC problem using sequential convex programming for trajectory generation and control. We prove that in the unconstrained case, the optimal value of the DNOC converges to that of SNOC asymptotically and that any feasible solution of the constrained DNOC is a feasible solution of the chance-constrained SNOC. We also present the predictor-corrector extension (gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$) for real-time motion trajectory generation in the presence of stochastic uncertainty. In the gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$ method, we first predict the uncertainty using the gPC method and then optimize the motion plan to accommodate the uncertainty. We empirically demonstrate the efficacy of the gPC-SCP and the gPC-SCP\${\textasciicircum}{\textbackslash}mathrm\{PC\}\$ methods for the following two test cases: 1) collision checking under uncertainty in actuation and physical parameters and 2) collision checking with stochastic obstacle model for 3DOF and 6DOF robotic systems. We validate the effectiveness of the gPC-SCP method on the 3DOF robotic spacecraft testbed.},
  urldate   = {2023-06-05},
  publisher = {arXiv},
  author    = {Nakka, Yashwanth Kumar and Chung, Soon-Jo},
  month     = mar,
  year      = {2022},
  note      = {arXiv:2106.02801 [cs, eess, math]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control}
}

@book{puterman_markov_1994,
  edition    = {1},
  series     = {Wiley {Series} in {Probability} and {Statistics}},
  title      = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
  isbn       = {978-0-471-61977-2 978-0-470-31688-7},
  shorttitle = {Markov {Decision} {Processes}},
  url        = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
  language   = {en},
  urldate    = {2023-05-26},
  publisher  = {Wiley},
  author     = {Puterman, Martin L.},
  month      = apr,
  year       = {1994},
  doi        = {10.1002/9780470316887}
}

@misc{brockman_openai_2016-1,
  title     = {{OpenAI} {Gym}},
  url       = {http://arxiv.org/abs/1606.01540},
  abstract  = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  urldate   = {2023-05-26},
  publisher = {arXiv},
  author    = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1606.01540 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@misc{schulman_proximal_2017,
  title     = {Proximal {Policy} {Optimization} {Algorithms}},
  url       = {http://arxiv.org/abs/1707.06347},
  abstract  = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  urldate   = {2023-05-26},
  publisher = {arXiv},
  author    = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  month     = aug,
  year      = {2017},
  note      = {arXiv:1707.06347 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@misc{pardo_time_2022,
  title     = {Time {Limits} in {Reinforcement} {Learning}},
  url       = {http://arxiv.org/abs/1712.00378},
  abstract  = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
  urldate   = {2023-05-26},
  publisher = {arXiv},
  author    = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
  month     = jan,
  year      = {2022},
  note      = {arXiv:1712.00378 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@article{bjelonic_learning-based_2023,
  title    = {Learning-based {Design} and {Control} for {Quadrupedal} {Robots} with {Parallel}-{Elastic} {Actuators}},
  volume   = {8},
  issn     = {2377-3766, 2377-3774},
  url      = {http://arxiv.org/abs/2301.03509},
  doi      = {10.1109/LRA.2023.3234809},
  abstract = {Parallel-elastic joints can improve the efficiency and strength of robots by assisting the actuators with additional torques. For these benefits to be realized, a spring needs to be carefully designed. However, designing robots is an iterative and tedious process, often relying on intuition and heuristics. We introduce a design optimization framework that allows us to co-optimize a parallel elastic knee joint and locomotion controller for quadrupedal robots with minimal human intuition. We design a parallel elastic joint and optimize its parameters with respect to the efficiency in a model-free fashion. In the first step, we train a design-conditioned policy using model-free Reinforcement Learning, capable of controlling the quadruped in the predefined range of design parameters. Afterwards, we use Bayesian Optimization to find the best design using the policy. We use this framework to optimize the parallel-elastic spring parameters for the knee of our quadrupedal robot ANYmal together with the optimal controller. We evaluate the optimized design and controller in real-world experiments over various terrains. Our results show that the new system improves the torque-square efficiency of the robot by 33\% compared to the baseline and reduces maximum joint torque by 30\% without compromising tracking performance. The improved design resulted in 11\% longer operation time on flat terrain.},
  number   = {3},
  urldate  = {2023-05-25},
  journal  = {IEEE Robotics and Automation Letters},
  author   = {Bjelonic, Filip and Lee, Joonho and Arm, Philip and Sako, Dhionis and Tateo, Davide and Peters, Jan and Hutter, Marco},
  month    = mar,
  year     = {2023},
  note     = {arXiv:2301.03509 [cs]},
  keywords = {Computer Science - Robotics},
  pages    = {1611--1618}
}

@article{song_policy_2022,
  title   = {Policy {Search} for {Model} {Predictive} {Control} {With} {Application} to {Agile} {Drone} {Flight}},
  volume  = {38},
  issn    = {1552-3098, 1941-0468},
  url     = {https://ieeexplore.ieee.org/document/9719129/},
  doi     = {10.1109/TRO.2022.3141602},
  number  = {4},
  urldate = {2023-05-25},
  journal = {IEEE Transactions on Robotics},
  author  = {Song, Yunlong and Scaramuzza, Davide},
  month   = aug,
  year    = {2022},
  pages   = {2114--2130}
}

@techreport{traversaro_multibody_2019,
  title       = {Multibody dynamics notation (version 2)},
  shorttitle  = {Multibody dynamics notation},
  url         = {https://research.tue.nl/en/publications/multibody-dynamics-notation-version-2},
  abstract    = {This document provides a revision of the notation originally introduced in [20] for describing kinematics and dynamics quantities of mechanical systems composed by several rigid bodies. Relative to the first edition, this new version includes an expanded section on frame acceleration (Section 5.4), the correction of a few typos, and the change of the fonts used in the notation from single face to bold face.
                 The notation detailed in this document is inspired by the well-known Featherstone notation introduced in [7], also used, with small adaptations, in the Handbook of Robotics [16]. Featherstone’s notation, while being extremely compact and pleasant for the eye, is not fully in accordance with Lie group formalism, with the potential of creating a misunderstanding between the robotics and geometric mechanics communities.
                 The Lie group formalism is well established in the robotics literature [13, 14,
                 10]. However, it is less compact than Featherstone’s notation [7], leading to long expressions when several rigid bodies are present as in the case of a complete dynamic model of humanoid or quadruped robots. This report aims, therefore, at getting the best from these two worlds. The notation strives to be compact, precise, and in harmony with Lie Group formalism. The document furthermore introduces a flexible and unambiguous notation
                 to describe the Jacobians mapping generalized velocities of an arbitrary frame to Cartesian linear and angular velocities, expressed with respect to a reference frame of choice.},
  language    = {English},
  institution = {Technische Universiteit Eindhoven},
  author      = {Traversaro, Silvio and Saccon, Alessandro},
  year        = {2019},
  keywords    = {lie group, multibody dynamics, robot, robotics},
  pages       = {22}
}

@misc{schulman_proximal_2017-1,
  title     = {Proximal {Policy} {Optimization} {Algorithms}},
  url       = {http://arxiv.org/abs/1707.06347},
  abstract  = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  urldate   = {2023-05-25},
  publisher = {arXiv},
  author    = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  month     = aug,
  year      = {2017},
  note      = {arXiv:1707.06347 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@article{ha_reinforcement_2019,
  title    = {Reinforcement {Learning} for {Improving} {Agent} {Design}},
  volume   = {25},
  issn     = {1064-5462, 1530-9185},
  url      = {http://arxiv.org/abs/1810.03779},
  doi      = {10.1162/artl_a_00301},
  abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications. Videos of results at https://designrl.github.io/},
  number   = {4},
  urldate  = {2023-05-25},
  journal  = {Artificial Life},
  author   = {Ha, David},
  month    = nov,
  year     = {2019},
  note     = {arXiv:1810.03779 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  pages    = {352--365}
}

@misc{tobin_domain_2017,
  title     = {Domain {Randomization} for {Transferring} {Deep} {Neural} {Networks} from {Simulation} to the {Real} {World}},
  url       = {http://arxiv.org/abs/1703.06907},
  abstract  = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  urldate   = {2023-05-25},
  publisher = {arXiv},
  author    = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  month     = mar,
  year      = {2017},
  note      = {arXiv:1703.06907 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Robotics}
}

@article{sims_evolving_1994,
  title    = {Evolving {3D} {Morphology} and {Behavior} by {Competition}},
  volume   = {1},
  issn     = {1064-5462, 1530-9185},
  url      = {https://direct.mit.edu/artl/article/1/4/353-372/2234},
  doi      = {10.1162/artl.1994.1.4.353},
  abstract = {This article describes a system for the evolution and coevolution of virtual creatures that compete in physically simulated three-dimensional worlds. Pairs of individuals enter one-on-one contests in which they contend to gain control of a common resource. The winners receive higher relative fitness scores allowing them to survive and reproduce. Realistic dynamics simulation including gravity, collisions, and friction, restricts the actions to physically plausible behaviors.
              The morphology of these creatures and the neural systems for controlling their muscle forces are both genetically determined, and the morphology and behavior can adapt to each other as they evolve simultaneously. The genotypes are structured as directed graphs of nodes and connections, and they can efficiently but flexibly describe instructions for the development of creatures' bodies and control systems with repeating or recursive components. When simulated evolutions are performed with populations of competing creatures, interesting and diverse strategies and counterstrategies emerge.},
  language = {en},
  number   = {4},
  urldate  = {2023-05-25},
  journal  = {Artificial Life},
  author   = {Sims, Karl},
  month    = jul,
  year     = {1994},
  pages    = {353--372}
}

@article{rojas_easy_2022,
  title    = {An {Easy} to {Use} {Deep} {Reinforcement} {Learning} {Library} for {AI} {Mobile} {Robots} in {Isaac} {Sim}},
  volume   = {12},
  issn     = {2076-3417},
  url      = {https://www.mdpi.com/2076-3417/12/17/8429},
  doi      = {10.3390/app12178429},
  abstract = {The use of mobile robots for personal and industrial uses is becoming popular. Currently, many robot simulators with high-graphical capabilities can be used by engineering to develop and test these robots such as Isaac Sim. However, using that simulator to train mobile robots with the deep reinforcement learning paradigm can be very difficult and time-consuming if one wants to develop a custom experiment, requiring an understanding of several libraries and APIs to use them together correctly. The proposed work aims to create a library that conceals configuration problems in creating robots, environments, and training scenarios, reducing the time dedicated to code. Every developed method is equivalent to sixty-five lines of code at maximum and five at minimum. That brings time saving in simulated experiments and data collection, thus reducing the time to produce and test viable algorithms for robots in the industry or academy.},
  language = {en},
  number   = {17},
  urldate  = {2023-05-25},
  journal  = {Applied Sciences},
  author   = {Rojas, Maximiliano and Hermosilla, Gabriel and Yunge, Daniel and Farias, Gonzalo},
  month    = aug,
  year     = {2022},
  pages    = {8429}
}

@misc{chen_hardware_2020,
  title      = {Hardware as {Policy}: {Mechanical} and {Computational} {Co}-{Optimization} using {Deep} {Reinforcement} {Learning}},
  shorttitle = {Hardware as {Policy}},
  url        = {http://arxiv.org/abs/2008.04460},
  abstract   = {Deep Reinforcement Learning (RL) has shown great success in learning complex control policies for a variety of applications in robotics. However, in most such cases, the hardware of the robot has been considered immutable, modeled as part of the environment. In this study, we explore the problem of learning hardware and control parameters together in a unified RL framework. To achieve this, we propose to model the robot body as a "hardware policy", analogous to and optimized jointly with its computational counterpart. We show that, by modeling such hardware policies as auto-differentiable computational graphs, the ensuing optimization problem can be solved efficiently by gradient-based algorithms from the Policy Optimization family. We present two such design examples: a toy mass-spring problem, and a real-world problem of designing an underactuated hand. We compare our method against traditional co-optimization approaches, and also demonstrate its effectiveness by building a physical prototype based on the learned hardware parameters. Videos and more details are available at https://roamlab.github.io/hwasp/ .},
  urldate    = {2023-05-25},
  publisher  = {arXiv},
  author     = {Chen, Tianjian and He, Zhanpeng and Ciocarlie, Matei},
  month      = nov,
  year       = {2020},
  note       = {arXiv:2008.04460 [cs]},
  keywords   = {Computer Science - Robotics}
}

@misc{schulman_trust_2017,
  title     = {Trust {Region} {Policy} {Optimization}},
  url       = {http://arxiv.org/abs/1502.05477},
  abstract  = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  urldate   = {2023-05-25},
  publisher = {arXiv},
  author    = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  month     = apr,
  year      = {2017},
  note      = {arXiv:1502.05477 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@misc{wang_sample_2017,
  title     = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
  url       = {http://arxiv.org/abs/1611.01224},
  abstract  = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  urldate   = {2023-05-25},
  publisher = {arXiv},
  author    = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  month     = jul,
  year      = {2017},
  note      = {arXiv:1611.01224 [cs]},
  keywords  = {Computer Science - Machine Learning}
}

@book{sutton_reinforcement_1998,
  address    = {Cambridge, Mass},
  series     = {Adaptive computation and machine learning},
  title      = {Reinforcement learning: an introduction},
  isbn       = {978-0-262-19398-6},
  shorttitle = {Reinforcement learning},
  publisher  = {MIT Press},
  author     = {Sutton, Richard S. and Barto, Andrew G.},
  year       = {1998},
  keywords   = {Reinforcement learning}
}
